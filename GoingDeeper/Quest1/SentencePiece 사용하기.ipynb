{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3ee1f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  1 05:59:14 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   58C    P0    28W /  70W |    898MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# GPU로 Tesla T4가 준비돼있습니다.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f5995890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "3.4.3\n",
      "0.5.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import konlpy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(plt.__version__)\n",
    "print(konlpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d15550",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 가져오기 & 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f40fc4",
   "metadata": {},
   "source": [
    "## 1-1) 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4b1cca9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ac668",
   "metadata": {},
   "source": [
    "## 1-2) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "857e775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "cleaned_corpus = list(set(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb931ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10~150 범위의 문장만 선택 나머지는 제거\n",
    "max_len = 150\n",
    "min_len = 10\n",
    "\n",
    "# 길이 조건에 맞는 문장만 선택합니다.\n",
    "filtered_corpus = [s for s in cleaned_corpus \n",
    "                   if (len(s) < max_len) & (len(s) >= min_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784eef0",
   "metadata": {},
   "source": [
    "# Step 2. SentencePiece 설치하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df082b8a",
   "metadata": {},
   "source": [
    "## 2-1) tokenize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "98c2cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6ee2",
   "metadata": {},
   "source": [
    "## 2-2) SentencePiece 모델을 학습 및 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6e87c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14593 obj=14.7196 num_tokens=705636 num_tokens/piece=48.3544\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14593 obj=14.648 num_tokens=705645 num_tokens/piece=48.355\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10944 obj=15.0875 num_tokens=741620 num_tokens/piece=67.765\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10944 obj=15.007 num_tokens=741624 num_tokens/piece=67.7654\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3757 num_tokens=769363 num_tokens/piece=87.4276\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.307 num_tokens=769367 num_tokens/piece=87.4281\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 376816 Jul  1 05:59 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 Jul  1 05:59 korean_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d4c58709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1243, 11, 302, 7, 3608, 11, 287, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22bd74",
   "metadata": {},
   "source": [
    "# Step 3. Tokenizer 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d026a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus): \n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5f405",
   "metadata": {},
   "source": [
    "# Step 4. 네이버 영화리뷰 감정 분석 문제에 SentencePiece 적용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682f7dc",
   "metadata": {},
   "source": [
    "## 4-1) 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "616c30ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 200001\n",
      "Example:\n",
      ">> id\tdocument\tlabel\n",
      ">> 9468781\t어?생각없이 봤는데 상당한 수작.일본영화 10년내 최고로 마음에 들었다.강렬한 임팩트가 일품.\t1\n",
      ">> 6809191\t마치 바다속 , 아쿠아리움속으로 들어간듯한 느낌의 영화 어린자녀들에게 강추!!\t1\n",
      ">> 8255656\t재밌습니다.재밌습니다.\t1\n",
      ">> 8499726\t너무너무재밌게보고있어요! 중간부터 본방사수했지만 스토리가 탄탄하고 흥미진진해서 1화부터 다찾아서 봤네요 배우들 연기도 어디하나빠지는데가없네요 요새는 수백향만챙겨봐요\t1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_file = 'NaverReviewData/ratings.txt'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d089d",
   "metadata": {},
   "source": [
    "## 4-2) 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cc28247e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 8\n",
      "문장의 최장 길이: 156\n",
      "문장의 평균 길이: 45\n",
      "데이터 총량: 200001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "min_len = 999 \n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in raw:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(raw))\n",
    "print(\"데이터 총량:\",len(raw))\n",
    "\n",
    "#각 길이를 가진 문장의 개수 저장\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in raw:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "#X 문장길이 / Y : 문장의 개수    \n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d80832",
   "metadata": {},
   "source": [
    "## 4-3) SentencePiece 적용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "28461f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "def sp_tokenize(s, corpus): \n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus[1:]:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word\n",
    "\n",
    "tensor, word_index, index_word = sp_tokenize(s, raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e59584bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5f16bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 159, 2820, 1715, 3630,  549,  666, 1817,  146,   14,  679, 4609,\n",
       "       6173,   30,  295,    0,   71,  514,    0,   51,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fa23c",
   "metadata": {},
   "source": [
    "## 4-4) 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "de3bbcbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for line in raw[1:]:\n",
    "    label = line.split('\\t')[2]  # 탭 문자로 분리 후 첫 번째 요소(레이블) 선택\n",
    "    labels.append(int(label))  # 레이블을 정수형으로 변환하여 리스트에 추가\n",
    "\n",
    "print(len(labels))  # 처음 10개 레이블 출력\n",
    "\n",
    "tensor_train = tensor[:200000]\n",
    "labels_train = labels[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e4607cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data와 Test data 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tensor_train, \n",
    "                                                    labels_train, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify = labels_train, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "afff6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype=np.float32)  # 레이블 데이터 타입 변환\n",
    "y_test = np.array(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3f9325c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80000, 80000])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, counts = np.unique(y_train,return_counts = True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bec2bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20000, 20000])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, counts = np.unique(y_test,return_counts = True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb532b1",
   "metadata": {},
   "source": [
    "## 4-5)모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "da94840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding # Embedding 레이어 직접 import\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                             output_dim=128))\n",
    "model.add(tf.keras.layers.LSTM(32))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1284d4bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 33s 6ms/step - loss: 0.6933 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d1cfe604f10>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e750f8",
   "metadata": {},
   "source": [
    "# Step 5. KoNLPy 형태소 분석기를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f888d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def mecab_split(sentence):\n",
    "    return mecab.morphs(sentence)\n",
    "\n",
    "mecab_corpus = []\n",
    "\n",
    "for kor in raw[1:]:\n",
    "    mecab_corpus.append(mecab_split(kor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3053dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeCab Vocab Size: 261053\n"
     ]
    }
   ],
   "source": [
    "mecab_tensor, mecab_tokenizer = tokenize(mecab_corpus)\n",
    "\n",
    "print(\"MeCab Vocab Size:\", len(mecab_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4e142d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mecab_tensor, \n",
    "                                                    labels_train, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify = labels_train, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bc1fdb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, dtype=np.float32)  # 레이블 데이터 타입 변환\n",
    "y_test = np.array(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "77fd1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                             output_dim=128))\n",
    "model.add(tf.keras.layers.SimpleRNN(32))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "05296bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b3d44f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 401s 80ms/step - loss: 0.4020 - accuracy: 0.8057 - val_loss: 0.2824 - val_accuracy: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d1d22801430>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63500dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
