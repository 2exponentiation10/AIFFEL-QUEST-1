{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932e65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaed51f",
   "metadata": {},
   "source": [
    "# 1. 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4fcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_kor_file = \"data/korean-english-park.train.ko\"\n",
    "path_eng_file = \"data/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6563dd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "with open(path_kor_file, \"r\") as f:\n",
    "    kor_raw = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size:\", len(kor_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in kor_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7543afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(path_eng_file, \"r\") as f:\n",
    "    eng_raw = f.read().splitlines()\n",
    "    \n",
    "print(\"Data Size:\", len(eng_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in eng_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5fa27e",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리 & 토큰화\n",
    "\n",
    "Step 2. 데이터 정제\n",
    "\n",
    "1. set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "2. 앞서 정의한 preprocessing() 함수는 한글에서는 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!\n",
    "3. 타겟 언어인 영문엔 \\<start\\> 토큰과 \\<end\\> 토큰을 추가하고 split() 함수를 이용하여 토큰화합니다. 한글 토큰화는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 토큰의 길이가 40 이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5056c",
   "metadata": {},
   "source": [
    "## 2-1) 중복 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8149ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#중복 제거\n",
    "unique_pairs = set(zip(kor_raw, eng_raw))#쌍튜플을 set자료형 변환\n",
    "\n",
    "#리스트 변환\n",
    "kor_lines, eng_lines = zip(*unique_pairs) #튜플 unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2e7602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('그러나 그는 다이애나비의 사생활에 대한 상세한 심리가 있을 예정이라고 전했다.', '이 명령 또한 고든 판사가 내렸으며 고든 판사는 당시 구체적인 설명을 하지 않았고 특정 약물 이름도 언급하지 않았다.', '최근 멸종위기에 놓인 이 회색곰과 늑대는 다른 위기에 빠진 종에 밀려 내년중으로 멸종 위기에 놓인 종 리스트에서 빠질 것으로 예상되고 있다.', '이 같은 북측의 긴장감 조성을 위한 발언은 종종 있었다.', '코소보의 서부 정책은 지역 분쟁의 이유가 될 수 있는 것들을 아예 배제시켰다.')\n",
      "('But he said they would hear \"intimate\" details of her personal life.', 'That order, also by Gordon, provided no details and did not name any drugs.', 'With grizzlies in and around Yellowstone National Park recently taken off the threatened species list and gray wolves expected to come off the endangered list within the next year, that spending likely will drop as the agency turns its resources to other imperiled species.', 'On Friday, North Korea test-fired a barrage of missiles into the sea and warned it would \"mercilessly wipe out\" any South Korean warships that violate its waters near their disputed sea border.', 'Western policy on Kosovo has previously ruled out partition as a potential spark for regional conflict.')\n"
     ]
    }
   ],
   "source": [
    "print(kor_lines[:5])\n",
    "print(eng_lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4c6c7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968 78968\n"
     ]
    }
   ],
   "source": [
    "print(len(kor_lines), len(eng_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02ac6e",
   "metadata": {},
   "source": [
    "## 2-2) 길이가 40 이하인 데이터 선별 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f8b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40 #최대길이\n",
    "\n",
    "filtered_kor_lines, filtered_eng_lines = [], []\n",
    "\n",
    "for koc, enc in zip(kor_lines, eng_lines):\n",
    "    if len(koc.split()) <= max_len and len(enc.split()) <= max_len:\n",
    "        filtered_kor_lines.append(koc)\n",
    "        filtered_eng_lines.append(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ed7a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75577 75577\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_kor_lines), len(filtered_eng_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab634ec",
   "metadata": {},
   "source": [
    "## 2-3)한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c5f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False, korean = True):\n",
    "    sentence = sentence.lower().strip() #소문자 변환\n",
    "    \n",
    "    regex = re.compile(r\"([?.!,])\")\n",
    "    sentence = regex.sub(r\" \\1 \", sentence)\n",
    "    #\\1 : 일치하는 특수 문자(?, !, .)를 나타냄\n",
    "    # r\" \\1 \"는 일치하는 특수 문자 앞뒤에 공백을 추가\n",
    "    \n",
    "    regex = re.compile(r'[\" \"]+')\n",
    "    sentence = regex.sub(\" \", sentence) #불필요한 공백 제거\n",
    "    \n",
    "    #한국어와 영어의 데이터 전처리\n",
    "    if korean:\n",
    "        regex = re.compile(r\"[^가-힣?.!,]+\")\n",
    "        sentence = regex.sub(\" \", sentence)#불필요한 문자 제거\n",
    "    else :\n",
    "        sentence = sentence.lower().strip() #소문자 변환\n",
    "        regex = re.compile(r\"[^a-zA-Z?.!,]+\")\n",
    "        sentence = regex.sub(\" \", sentence)#불필요한 문자 제거\n",
    "\n",
    "    sentence = sentence.strip()#공백제거\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa920d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 이 같은 북측의 긴장감 조성을 위한 발언은 종종 있었다 .\n",
      "English: <start> on friday , north korea test fired a barrage of missiles into the sea and warned it would mercilessly wipe out any south korean warships that violate its waters near their disputed sea border . <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 20000\n",
    "\n",
    "for kor,eng in zip(filtered_kor_lines[:num_examples], \n",
    "                   filtered_eng_lines[:num_examples]):\n",
    "    enc_corpus.append(preprocess_sentence(kor))\n",
    "    dec_corpus.append(preprocess_sentence(eng, \n",
    "                                          s_token=True, \n",
    "                                          e_token=True,\n",
    "                                          korean = False))\n",
    "\n",
    "\n",
    "print(\"Korean:\", enc_corpus[2])\n",
    "print(\"English:\", dec_corpus[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78771edf",
   "metadata": {},
   "source": [
    "## 2-4) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac7c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def enc_tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
    "                                                      filters='')\n",
    "    mecab = Mecab()\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    for sen in corpus:\n",
    "        tokenized_sen = mecab.morphs(sen) #토큰화\n",
    "        tokenized_corpus.append(tokenized_sen)\n",
    "        \n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(tokenized_corpus)  # 정수 인코딩\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, \n",
    "                                                                     padding='post')  # 패딩\n",
    "\n",
    "    return padded_sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19fd4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 토큰화 함수\n",
    "def dec_tokenize(corpus):\n",
    "    #토크나이저 객체생성, filters = ' ' :특수문자 필터링 비활성황 \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
    "                                                      filters='')\n",
    "    \n",
    "    #텍스트 말뭉치(corpus)에 등장하는 모든 단어를 분석하고 각 단어에 고유한 정수 인덱스를 부여\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    #텍스트 말뭉치를 정수 시퀀스로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c6a83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_tokenizer = enc_tokenize(enc_corpus)\n",
    "dec_train, dec_tokenizer = dec_tokenize(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ff9bd5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 81)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56b5105a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 58)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3629e33",
   "metadata": {},
   "source": [
    "# 3. 모델설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5008aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ab4fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8e44615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55e88249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 54, 1024)\n",
      "Decoder Output: (64, 25648)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 54, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 54\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e445c",
   "metadata": {},
   "source": [
    "# 4. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee4700",
   "metadata": {},
   "source": [
    "## 4-1) train, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "190b27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf052b",
   "metadata": {},
   "source": [
    "## 4-2) train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95851955",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c94acc",
   "metadata": {},
   "source": [
    "## 4-3) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7f31e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 313/313 [06:59<00:00,  1.34s/it, Loss 3.0031]\n",
      "Epoch  2: 100%|██████████| 313/313 [05:26<00:00,  1.04s/it, Loss 2.9138]\n",
      "Epoch  3: 100%|██████████| 313/313 [05:28<00:00,  1.05s/it, Loss 2.8382]\n",
      "Epoch  4: 100%|██████████| 313/313 [05:28<00:00,  1.05s/it, Loss 2.8030]\n",
      "Epoch  5: 100%|██████████| 313/313 [05:28<00:00,  1.05s/it, Loss 2.7801]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24698972",
   "metadata": {},
   "source": [
    "# 5. 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d177aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 식사 하셨습니까 ? ?\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226/1099797530.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_226/1099797530.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 49885 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 54616 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 49512 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 44620 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 49885 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 49324 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 54616 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 49512 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 49845 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 45768 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 44620 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"식사 하셨습니까??\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99b7aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"오바마는 대통령이다.\",\n",
    "             \"시민들은 도시 속에 산다.\", \n",
    "             \"커피는 필요 없다.\",\n",
    "             \"일곱 명의 사망자가 발생했다.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbbe3dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    }
   ],
   "source": [
    "translate(sentences[0], encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bd44991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Input: 커피는 필요 없다 .\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Predicted translation: the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the \n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "for sen in sentences:\n",
    "    translate(sen, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091f4d3",
   "metadata": {},
   "source": [
    "# 6. 결과 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c7aba",
   "metadata": {},
   "source": [
    "## 6-1) 첫번째 시도 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29691380",
   "metadata": {},
   "source": [
    "Input: 오바마는 대통령이다 .\n",
    "\n",
    "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2b0a2",
   "metadata": {},
   "source": [
    "- LMS 코드와 동일한 프로세스로 진행을 하였지만, 코드가 위와같이 작동하지 않았음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e96ba5",
   "metadata": {},
   "source": [
    "학습 데이터량 10000 > 20000 증강 후 재 테스트 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e2aed",
   "metadata": {},
   "source": [
    "## 6-2) 두번째 시도 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b052a7b",
   "metadata": {},
   "source": [
    "- Input: 오바마는 대통령이다 .\n",
    "\n",
    "Predicted translation: books processors exploration betina cor sprung hopeless magnetic rig dmitry behaved cano telescopes gawker mesopotamia canberra strafe hauled fidel angie maintenance enters simba bands skibo shotaro probation gravest differently deteriorates morici bulb utterly divine pedro coal separatist toledo lucinda mike bergen swiveling superstar trimmed rodriguez applied global dissatisfaction constable looped varshavyanka \n",
    "- Input: 시민들은 도시 속에 산다 .\n",
    "\n",
    "Predicted translation: books processors exploration betina cor sprung hopeless magnetic rig dmitry behaved cano telescopes gawker mesopotamia canberra strafe hauled fidel angie maintenance enters simba bands skibo shotaro probation gravest differently deteriorates morici bulb utterly divine pedro coal separatist toledo lucinda mike bergen swiveling superstar trimmed rodriguez applied global dissatisfaction constable looped varshavyanka \n",
    "- Input: 커피는 필요 없다 .\n",
    "\n",
    "Predicted translation: books processors exploration betina cor sprung hopeless magnetic rig dmitry behaved cano telescopes gawker mesopotamia canberra strafe hauled fidel angie maintenance enters simba bands skibo shotaro probation gravest differently deteriorates morici bulb utterly divine pedro coal separatist toledo lucinda mike bergen swiveling superstar trimmed rodriguez applied global dissatisfaction constable looped varshavyanka \n",
    "- Input: 일곱 명의 사망자가 발생했다 .\n",
    "\n",
    "Predicted translation: books processors exploration betina cor sprung hopeless magnetic rig dmitry behaved cano telescopes gawker mesopotamia canberra strafe hauled fidel angie maintenance enters simba bands skibo shotaro probation gravest differently deteriorates morici bulb utterly divine pedro coal separatist toledo lucinda mike bergen swiveling superstar trimmed rodriguez applied global dissatisfaction constable looped varshavyanka "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99f51e",
   "metadata": {},
   "source": [
    "- 구조적인 문제가 있는 것으로 예상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9c29e",
   "metadata": {},
   "source": [
    "# 6-3) 세번째 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358f660",
   "metadata": {},
   "source": [
    "- 학습 데이터량을 20000으로 고정하고, 단어사전의 개수를 10000 -> 15000으로 진행\n",
    "- epoch 수 5-> 10 변경후 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912c47a",
   "metadata": {},
   "source": [
    "- Input: 오바마는 대통령이다 .\n",
    "- Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
    "- Input: 시민들은 도시 속에 산다 .\n",
    "- Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
    "- Input: 커피는 필요 없다 .\n",
    "- Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
    "- Input: 일곱 명의 사망자가 발생했다 .\n",
    "- Predicted translation: the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the government , the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854ae23",
   "metadata": {},
   "source": [
    "### 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011da1d",
   "metadata": {},
   "source": [
    "1. 배운점\n",
    "- attention 모델의 종류와 모델 구성에 대해서 알게 되었다\n",
    "- SeqtoSeq 모델의 학습속도가 너무 느리다는 것을 알게되었다.\n",
    "\n",
    "2. 아쉬운점\n",
    "- 학습속도가 너무 느려서 다양한 하이퍼파라미터에 대한 학습과정을 test해보지 못한것\n",
    "\n",
    "3. 느낀점\n",
    "- 수학적으로는 이해하기 어려웠는데 코드로 접근하니까 이해할 수 있었던 부분이 많았다.\n",
    "\n",
    "4. 어려웠던점\n",
    "- 모델구현이 생각보다 너무 안되서 해결해보고 싶었지만 학습 시간과 GPU의 한계로 그러지 못하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee486b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
